# AWS Bedrock Fine-Tuning Automation â€” Beginner's Complete Guide

> **Who is this for?** Absolute beginners to AI and Cloud. No prior AWS or machine learning experience required.  
> **Time Commitment:** ~8â€“10 hours across 2 weeks (broken into manageable sessions)  
> **Cost Estimate:** ~$15â€“$40 USD in AWS charges (we'll show you how to keep costs low)  
> **Last Updated:** February 2026

---

## What You'll Learn (And Why It Matters)

Imagine you hire a brilliant new employee. They're smart, but they don't know *your* company's language, products, or customers yet. **Fine-tuning** is like giving that employee specialised on-the-job training so they become an expert in *your* specific domain.

In this guide, you'll learn how to:

1. **Prepare training data** â€” Create the "textbook" your AI model will learn from
2. **Fine-tune an AI model** â€” Teach Amazon Bedrock's foundation models your specific use case
3. **Automate the entire process** â€” Build pipelines that retrain your model automatically when new data arrives
4. **Evaluate the results** â€” Scientifically compare "before" and "after" to prove your fine-tuning worked

By the end, you'll have a fully automated AI training pipeline â€” the same kind of system enterprise teams build in production.

---

## Prerequisites

### Accounts & Access

| Requirement | What It Is | How to Get It |
|---|---|---|
| **AWS Account** | Your cloud workspace | [Create a free account](https://aws.amazon.com/free/) |
| **AWS CLI** | Command-line tool to talk to AWS | [Installation guide](https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html) |
| **Python 3.9+** | Programming language we'll use | [Download Python](https://www.python.org/downloads/) |
| **Bedrock Model Access** | Permission to use AI models | Automatic (verified in setup below) |

### Concepts You Should Know (We'll Explain as We Go)

Don't worry if these terms are new â€” think of this as a preview, not a test:

- **Foundation Model** â€” A pre-trained AI model (like Claude or Titan) that already knows a lot, but hasn't been customised for your needs. Think of it as a university graduate who is smart but hasn't started their first job yet.
- **Fine-Tuning** â€” Teaching that foundation model your specific domain. Like that graduate completing a 3-month internship at your company.
- **Pipeline** â€” A series of automated steps that run in order, like a factory assembly line. Data goes in one end, a trained model comes out the other.
- **Orchestration** â€” Coordinating multiple steps so they run in the right order, handle errors, and notify you when done. Think of it as the factory floor manager.

---

## Environment Setup

### Step 1 â€” Install Python Dependencies

Open your terminal (Command Prompt on Windows, Terminal on Mac/Linux) and run:

```bash
# Create a project folder
mkdir bedrock-finetuning-lab
cd bedrock-finetuning-lab

# Create a virtual environment (keeps your project isolated â€” like a clean workspace)
python -m venv venv

# Activate it
# On Mac/Linux:
source venv/bin/activate
# On Windows:
venv\Scripts\activate

# Install the libraries we need
pip install boto3 rich
```

**What did we just install?**
- `boto3` â€” The official Python toolkit for talking to AWS services. Think of it as your translator between Python and the cloud.
- `rich` â€” Makes our terminal output colourful and easy to read (because learning should be fun).

### Step 2 â€” Configure AWS Credentials

```bash
aws configure
```

You'll be prompted to enter four things:

```
AWS Access Key ID:     [Your access key]
AWS Secret Access Key: [Your secret key]
Default region name:   ap-south-1
Default output format: json
```

**Where do I find my keys?** Go to the AWS Console â†’ Click your username (top right) â†’ Security credentials â†’ Create access key.

> **Security Note:** Treat your secret key like a password. Never share it, never commit it to GitHub, never paste it in a chat.

### Step 3 â€” Verify Bedrock Model Availability

As of late 2025, AWS Bedrock provides automatic access to foundation models. However, you should verify model availability and complete one-time requirements:

1. Open the [Amazon Bedrock Console](https://console.aws.amazon.com/bedrock/)
2. In the left sidebar, click **Model catalog**
3. Search for "Titan Text Express" 
4. Verify the model shows "Available" status
5. Search for "Claude 3.5 Sonnet"

> **Important for Anthropic Models**: When you first invoke an Anthropic model (Claude), AWS will prompt you to fill out a brief use case form. This is a **one-time requirement** and access is granted immediately after submission.

#### First-Time Anthropic Users Only

If you've never used Claude models in this AWS account:

1. In **Model catalog**, search for and select "Claude 3.5 Sonnet"
2. Click **Use in playground** (or try to invoke via API)
3. AWS will display: "Submit use case details for Anthropic"
4. Fill out the brief form:
   - Your name
   - Use case description (can be simple: "Learning and development")
   - Intended application type
5. Click **Submit**
6. Access is granted **immediately** (no approval waiting period)

> **Note**: This form submission at the root account is inherited by other accounts in the same AWS Organization. If you're using an organization's member account and someone has already submitted the form at the management account level, you won't need to resubmit.

**Model access is now automatic for most models â€” no manual enablement required!**

### Step 4 â€” Create an S3 Bucket for Training Data

S3 (Simple Storage Service) is like a filing cabinet in the cloud where we'll store our training data and model outputs.

```bash
# Replace 'your-unique-name' with something unique to you
aws s3 mb s3://bedrock-finetuning-your-unique-name --region ap-south-1
```

> **Naming tip:** S3 bucket names must be globally unique across ALL of AWS. Try including your name or a random number, like `bedrock-finetuning-jane-2026`.

### Step 5 â€” Verify Everything Works

Create a file called `verify_setup.py` and paste this in:

```python
import boto3
import json

def check_setup():
    print("=" * 50)
    print("  AWS Bedrock Fine-Tuning â€” Setup Verification")
    print("=" * 50)

    # Check 1: Can we connect to AWS?
    try:
        sts = boto3.client("sts")
        identity = sts.get_caller_identity()
        print(f"\nâœ… AWS Connection     : Working")
        print(f"   Account ID        : {identity['Account']}")
    except Exception as e:
        print(f"\nâŒ AWS Connection     : Failed â€” {e}")
        return

    # Check 2: Can we reach Bedrock?
    try:
        bedrock = boto3.client("bedrock", region_name="ap-south-1")
        models = bedrock.list_foundation_models()
        model_count = len(models.get("modelSummaries", []))
        print(f"âœ… Bedrock Access     : Working ({model_count} models available)")
    except Exception as e:
        print(f"âŒ Bedrock Access     : Failed â€” {e}")
        return

    # Check 3: Can we access S3?
    try:
        s3 = boto3.client("s3")
        buckets = s3.list_buckets()
        bucket_names = [b["Name"] for b in buckets["Buckets"]]
        ft_buckets = [b for b in bucket_names if "finetuning" in b.lower()]
        if ft_buckets:
            print(f"âœ… S3 Bucket          : Found â€” {ft_buckets[0]}")
        else:
            print(f"âš ï¸  S3 Bucket          : No fine-tuning bucket found (create one in Step 4)")
    except Exception as e:
        print(f"âŒ S3 Access          : Failed â€” {e}")

    print("\n" + "=" * 50)
    print("  Setup verification complete!")
    print("=" * 50)

if __name__ == "__main__":
    check_setup()
```

Run it:

```bash
python verify_setup.py
```

You should see green checkmarks (âœ…) for all three checks. If you see any red (âŒ), revisit the corresponding step above.

---

## Part 1 â€” Understanding Automating Fine-Tuning

Before we get hands-on, let's understand the building blocks. Each section below covers a key concept, followed by its hands-on lab.

### 1.1 â€” Step Functions for Orchestrating Training

**The Real-World Analogy:**  
Imagine you're baking a cake. You can't frost it before it's baked, and you can't bake it before mixing the ingredients. AWS Step Functions is your recipe card â€” it defines each step, the order they run in, and what to do if something goes wrong (like the oven breaking).

**What Step Functions Does for Fine-Tuning:**

Step Functions is an AWS service that lets you build **state machines** â€” visual workflows where each "state" is a step in your process. For fine-tuning, a typical workflow looks like this:

```
[Validate Data] â†’ [Start Fine-Tuning Job] â†’ [Wait for Completion] â†’ [Run Evaluation] â†’ [Register Model] â†’ [Notify Team]
```

Each box is a step. Step Functions handles:
- **Sequencing** â€” Steps run in the correct order, automatically
- **Error handling** â€” If a step fails, it can retry or take an alternative path
- **Waiting** â€” Fine-tuning can take hours. Step Functions patiently waits without you paying for idle compute
- **Visibility** â€” A visual dashboard shows you exactly which step is running, which passed, and which failed

**Key Terms:**
- **State Machine** â€” The complete workflow definition (your entire recipe)
- **State** â€” A single step in the workflow (one instruction in the recipe)
- **Task State** â€” A state that does work (like calling Bedrock to start fine-tuning)
- **Wait State** â€” A state that pauses (waiting for the fine-tuning job to finish)
- **Choice State** â€” A decision point (if evaluation score > 0.8, deploy; otherwise, alert the team)

### 1.2 â€” Using EventBridge for Automation

**The Real-World Analogy:**  
EventBridge is like setting up an automatic rule in your email: "When I receive an email from my boss with the word 'urgent', forward it to my phone." EventBridge watches for **events** (things that happen in AWS) and automatically **triggers actions** in response.

**What EventBridge Does for Fine-Tuning:**

Instead of manually clicking "start fine-tuning" every time you have new data, EventBridge can:

- **Detect new training data** â€” "When a new file is uploaded to my S3 training bucketâ€¦"
- **Run on a schedule** â€” "Every Sunday at 2 AM, check for new data and retrainâ€¦"
- **React to job completion** â€” "When a fine-tuning job finishes, start the evaluation pipelineâ€¦"

**Common EventBridge Patterns for Fine-Tuning:**

| Trigger | Action | Use Case |
|---|---|---|
| New file uploaded to S3 | Start Step Functions workflow | Automatic retraining on new data |
| Cron schedule (weekly) | Start Step Functions workflow | Regular retraining cadence |
| Bedrock job status change | Send SNS notification | Alert team when training finishes |
| Evaluation score below threshold | Trigger alert / rollback | Catch quality regressions |

**Key Terms:**
- **Event** â€” A record of something that happened ("file uploaded", "job completed")
- **Rule** â€” The condition you define ("when event X happens, do Y")
- **Target** â€” What gets triggered (Step Functions, Lambda, SNS, etc.)
- **Event Bus** â€” The central highway where events flow through

### 1.3 â€” Automated Evaluation Pipeline

**The Real-World Analogy:**  
After training a new employee, you don't just *hope* they learned â€” you give them a test. An automated evaluation pipeline is that test for your fine-tuned model. It runs automatically after every training job and tells you: "Did the model actually get better?"

**How Evaluation Works:**

```
[Fine-Tuning Complete] â†’ [Load Test Dataset] â†’ [Run Baseline Model] â†’ [Run Fine-Tuned Model] â†’ [Compare Scores] â†’ [Generate Report]
```

A good evaluation pipeline measures multiple dimensions:

- **Accuracy** â€” Does the model give correct answers?
- **Relevance** â€” Are the answers on-topic and useful?
- **Format Compliance** â€” Does the model follow your desired output format?
- **Safety** â€” Does the model still refuse harmful requests?

**Why Automate Evaluation?**  
Manual evaluation is slow, inconsistent, and doesn't scale. Automated evaluation runs the *exact same tests* every time, giving you comparable results across model versions. This is critical when you need to answer: "Is version 3 better than version 2?"

### 1.4 â€” Model Versioning & Model Registry Strategy

**The Real-World Analogy:**  
Think of this like version control for your AI models â€” just like how Google Docs saves every version of your document so you can go back to last Tuesday's version if today's edits were bad.

**Why Model Versioning Matters:**

Without versioning, you have questions like:
- "Which model is running in production right now?"
- "The model was better last week â€” can we go back?"
- "Which training data produced this model?"

**A Practical Versioning Strategy:**

```
models/
â”œâ”€â”€ banking-assistant-v1.0/     â† Baseline (original foundation model)
â”‚   â”œâ”€â”€ model-id.txt
â”‚   â”œâ”€â”€ training-config.json
â”‚   â””â”€â”€ evaluation-report.json
â”œâ”€â”€ banking-assistant-v1.1/     â† First fine-tune
â”‚   â”œâ”€â”€ model-id.txt
â”‚   â”œâ”€â”€ training-config.json
â”‚   â”œâ”€â”€ training-data-manifest.json
â”‚   â””â”€â”€ evaluation-report.json
â””â”€â”€ banking-assistant-v2.0/     â† Major improvement
    â”œâ”€â”€ model-id.txt
    â”œâ”€â”€ training-config.json
    â”œâ”€â”€ training-data-manifest.json
    â””â”€â”€ evaluation-report.json
```

**What to Track for Each Version:**

| Artefact | Why | Example |
|---|---|---|
| Model ID | So you can use it in Bedrock | `arn:aws:bedrock:ap-south-1:123456:custom-model/...` |
| Training Config | Reproducibility â€” retrain the same way | Epochs, learning rate, batch size |
| Training Data Reference | Know what the model learned from | S3 path + row count + date range |
| Evaluation Scores | Compare versions objectively | Accuracy: 87%, Relevance: 92% |
| Deployment Date | Audit trail | 2026-02-15T10:30:00Z |
| Rollback Pointer | Quick recovery if something goes wrong | "Previous version: v1.1" |

**Registry Pattern in Practice:**

We'll use a simple JSON-based registry stored in S3. Enterprise teams might use SageMaker Model Registry or a database, but the principle is identical: every model version gets a record with metadata.

---

## Part 2 â€” Hands-On Labs

Now let's build it. Each lab builds on the previous one, so complete them in order.

> **Pacing Tip:** Each lab is designed for a single focused session. Take breaks between labs â€” the concepts need time to settle.

---

### Lab 1 â€” Prepare a Fine-Tuning Dataset (Instruction-Tuning Style)

**Time:** ~1.5 hours  
**Goal:** Create a properly formatted training dataset for Amazon Bedrock fine-tuning  
**Scenario:** You're building a banking customer service assistant

#### What Is Instruction Tuning?

Standard fine-tuning says: "Here's a lot of text â€” learn from it."  
Instruction tuning says: "Here's a question â†’ here's the ideal answer. Learn to respond like this."

The format looks like this:

```json
{
    "prompt": "What is the process to dispute a credit card transaction?",
    "completion": "To dispute a credit card transaction, follow these steps: 1) Contact our support team within 60 days of the transaction date. 2) Provide the transaction date, amount, and merchant name. 3) We will issue a temporary credit while investigating. 4) Investigation typically completes within 10 business days. 5) You will receive written notification of the outcome."
}
```

Each line is one example. You're teaching the model: "When someone asks X, respond with Y."

#### Step-by-Step

**1. Create your dataset file:**

Create a file called `create_training_data.py`:

```python
import json
import os

# Our training examples â€” each one teaches the model how to handle
# a specific type of banking customer query.
training_examples = [
    {
        "prompt": "What is the process to dispute a credit card transaction?",
        "completion": "To dispute a credit card transaction: First, contact our support team within 60 days of the transaction date. Provide the transaction date, amount, and merchant name. We will issue a temporary credit to your account while we investigate. The investigation typically takes 10 business days, and you will receive written notification of the outcome via email or mail."
    },
    {
        "prompt": "How do I set up automatic bill payments?",
        "completion": "To set up automatic bill payments: Log into your online banking portal and navigate to the Bill Pay section. Click Add New Payee and enter the company name, your account number with them, and the payment amount. Choose your payment frequency (weekly, bi-weekly, or monthly) and the start date. Review the details and confirm. You can modify or cancel automatic payments at any time from the same section."
    },
    {
        "prompt": "What should I do if I suspect fraudulent activity on my account?",
        "completion": "If you suspect fraud on your account, take these immediate steps: Call our 24/7 fraud hotline to temporarily freeze your account. This prevents any further unauthorized transactions. Review your recent transaction history and note any unfamiliar charges. We will cancel your current card and issue a replacement within 3-5 business days. File a fraud report, which we will investigate within 48 hours. You are not liable for unauthorized transactions reported within 2 business days."
    },
    {
        "prompt": "How do I apply for a personal loan?",
        "completion": "To apply for a personal loan: You can start your application online, by phone, or at any branch. You will need to provide proof of identity, proof of income (recent pay stubs or tax returns), and your employment details. Loan amounts range from $1,000 to $50,000 with terms from 12 to 60 months. We typically provide a decision within 1 business day. If approved, funds are deposited into your account within 2-3 business days."
    },
    {
        "prompt": "What are the fees for international wire transfers?",
        "completion": "International wire transfer fees depend on the transfer method and destination. Outgoing international wires have a flat fee of $45 for transfers sent in USD and $35 for transfers in foreign currency. Incoming international wires have a fee of $15. Processing time is typically 1-3 business days. Expedited same-day transfers are available for an additional $25. Exchange rates are set at the time of transfer and include a small conversion margin."
    },
    {
        "prompt": "How can I increase my credit card limit?",
        "completion": "To request a credit limit increase: Log into your account online and navigate to Credit Card Services, then select Request Credit Limit Increase. You can also call the number on the back of your card. We will review your account history, payment patterns, and current income. A formal request may involve a soft credit inquiry that does not impact your credit score. Decisions are typically provided immediately or within 2 business days."
    },
    {
        "prompt": "What happens if I miss a mortgage payment?",
        "completion": "If you miss a mortgage payment, here is what to expect: A late fee is assessed if payment is not received within 15 days of the due date. After 30 days, the missed payment may be reported to credit bureaus, which can affect your credit score. Contact us immediately if you are having difficulty making payments because we offer hardship programs including temporary payment reduction, forbearance, and loan modification options. The sooner you reach out, the more options we can offer."
    },
    {
        "prompt": "How do I order new checks for my checking account?",
        "completion": "To order new checks: Log into your online banking and go to Account Services, then select Order Checks. You can choose from several check designs including standard, secure, and premium options. Standard checks are complimentary for premium account holders. For other accounts, pricing starts at $20 per box. Checks typically arrive within 7-10 business days. You can also order by calling customer service or visiting any branch."
    },
    {
        "prompt": "What is the difference between a savings account and a money market account?",
        "completion": "The key differences are: A savings account offers a fixed interest rate, has no minimum balance requirement for our basic tier, and allows up to 6 withdrawals per month. A money market account typically offers a higher interest rate that scales with your balance, requires a minimum balance of $2,500, and includes check-writing privileges and a debit card. Both accounts are FDIC insured up to $250,000. A money market account is better for larger balances where you want higher returns with some transaction flexibility."
    },
    {
        "prompt": "How do I set up two-factor authentication on my account?",
        "completion": "To enable two-factor authentication: Go to Settings in your online banking portal and select Security Preferences. Under Two-Factor Authentication, choose your preferred method: SMS text codes sent to your phone, authentication app (we support Google Authenticator and Authy), or a physical security key. Follow the on-screen prompts to verify your chosen method. Once enabled, you will need to provide a verification code each time you log in from an unrecognized device. We strongly recommend using an authentication app for the highest security."
    }
]

def create_dataset():
    """Create the JSONL training file for Bedrock fine-tuning."""

    output_file = "training_data.jsonl"

    print("=" * 55)
    print("  Banking Assistant â€” Training Data Generator")
    print("=" * 55)

    # Write each example as a separate JSON line (JSONL format)
    with open(output_file, "w") as f:
        for i, example in enumerate(training_examples):
            f.write(json.dumps(example) + "\n")
            print(f"  âœ… Example {i + 1:>2}: {example['prompt'][:50]}...")

    print(f"\n  ğŸ“ Created: {output_file}")
    print(f"  ğŸ“Š Total examples: {len(training_examples)}")
    print(f"  ğŸ“ File size: {os.path.getsize(output_file):,} bytes")
    print("\n" + "=" * 55)

    # Validate the file
    print("\n  Running validation checks...")
    with open(output_file, "r") as f:
        lines = f.readlines()
        for line_num, line in enumerate(lines, 1):
            data = json.loads(line.strip())
            assert "prompt" in data, f"Line {line_num}: Missing 'prompt' field"
            assert "completion" in data, f"Line {line_num}: Missing 'completion' field"
            assert len(data["prompt"]) > 0, f"Line {line_num}: Empty prompt"
            assert len(data["completion"]) > 0, f"Line {line_num}: Empty completion"

    print("  âœ… All validation checks passed!")
    print("  âœ… File format: JSONL (one JSON object per line)")
    print("  âœ… All examples have 'prompt' and 'completion' fields")

    return output_file

if __name__ == "__main__":
    create_dataset()
```

Run it:

```bash
python create_training_data.py
```

**2. Upload the dataset to S3:**

```bash
# Replace with your actual bucket name from Setup Step 4
aws s3 cp training_data.jsonl s3://bedrock-finetuning-your-unique-name/training-data/training_data.jsonl
```

**3. Verify the upload:**

```bash
aws s3 ls s3://bedrock-finetuning-your-unique-name/training-data/
```

You should see your `training_data.jsonl` file listed.

#### What You Just Accomplished

You created an **instruction-tuning dataset** in JSONL format â€” the standard format Bedrock expects. Each line contains a prompt-completion pair that teaches the model a specific banking interaction pattern.

> **Real-World Note:** Production datasets typically have 100â€“10,000+ examples. Our 10 examples are enough to demonstrate the process. More examples generally produce better results, but quality matters more than quantity. Ten excellent examples beat a thousand sloppy ones.

---

### Lab 2 â€” Trigger a Fine-Tuning Job (Bedrock Custom Model)

**Time:** ~1.5 hours (plus waiting time for the job to complete)  
**Goal:** Submit a fine-tuning job to Amazon Bedrock and monitor its progress  
**Prerequisite:** Lab 1 completed (training data uploaded to S3)

#### Important: IAM Permissions

Bedrock needs permission to read your training data from S3. We need to create an IAM role for this.

**1. Create the trust policy file** â€” `bedrock_trust_policy.json`:

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Principal": {
                "Service": "bedrock.amazonaws.com"
            },
            "Action": "sts:AssumeRole"
        }
    ]
}
```

**2. Create the S3 access policy file** â€” `bedrock_s3_policy.json`:

```json
{
    "Version": "2012-10-17",
    "Statement": [
        {
            "Effect": "Allow",
            "Action": [
                "s3:GetObject",
                "s3:ListBucket"
            ],
            "Resource": [
                "arn:aws:s3:::bedrock-finetuning-your-unique-name",
                "arn:aws:s3:::bedrock-finetuning-your-unique-name/*"
            ]
        }
    ]
}
```

> **Remember:** Replace `bedrock-finetuning-your-unique-name` with your actual bucket name.

**3. Create the IAM role:**

```bash
# Create the role
aws iam create-role \
    --role-name BedrockFineTuningRole \
    --assume-role-policy-document file://bedrock_trust_policy.json

# Attach the S3 access policy
aws iam put-role-policy \
    --role-name BedrockFineTuningRole \
    --policy-name BedrockS3Access \
    --policy-document file://bedrock_s3_policy.json
```

**4. Create the fine-tuning script** â€” `start_finetuning.py`:

```python
import boto3
import json
import time
from datetime import datetime

# â”€â”€â”€ CONFIGURATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Update these values to match your setup
REGION = "ap-south-1"
BUCKET_NAME = "bedrock-finetuning-your-unique-name"  # â† Change this!
ROLE_NAME = "BedrockFineTuningRole"
BASE_MODEL_ID = "amazon.titan-text-express-v1"
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_role_arn(role_name):
    """Look up the full ARN for our IAM role."""
    iam = boto3.client("iam")
    response = iam.get_role(RoleName=role_name)
    return response["Role"]["Arn"]

def start_fine_tuning_job():
    """Submit a fine-tuning job to Amazon Bedrock."""

    print("=" * 55)
    print("  Bedrock Fine-Tuning â€” Job Launcher")
    print("=" * 55)

    try:
        bedrock = boto3.client("bedrock", region_name=REGION)
        role_arn = get_role_arn(ROLE_NAME)

        # Generate a unique job name using timestamp
        timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
        job_name = f"banking-assistant-ft-{timestamp}"
        custom_model_name = f"banking-assistant-{timestamp}"

        print(f"\n  ğŸ“‹ Job Name         : {job_name}")
        print(f"  ğŸ§  Base Model       : {BASE_MODEL_ID}")
        print(f"  ğŸ“ Training Data    : s3://{BUCKET_NAME}/training-data/training_data.jsonl")
        print(f"  ğŸ”‘ IAM Role         : {role_arn}")
        print(f"  ğŸŒ Region           : {REGION}")

        # Define hyperparameters
        # These control HOW the model learns â€” don't worry about tuning
        # these for now; the defaults work well for learning purposes.
        hyper_parameters = {
            "epochCount": "2",          # How many times the model reads through all examples
            "batchSize": "1",           # How many examples to process at once
            "learningRate": "0.00001"   # How fast the model adjusts (smaller = more careful)
        }

        print(f"\n  âš™ï¸  Hyperparameters:")
        for key, value in hyper_parameters.items():
            print(f"     {key}: {value}")

        print(f"\n  ğŸš€ Submitting fine-tuning job...")
        response = bedrock.create_model_customization_job(
            jobName=job_name,
            customModelName=custom_model_name,
            roleArn=role_arn,
            baseModelIdentifier=BASE_MODEL_ID,
            trainingDataConfig={
                "s3Uri": f"s3://{BUCKET_NAME}/training-data/training_data.jsonl"
            },
            outputDataConfig={
                "s3Uri": f"s3://{BUCKET_NAME}/output/"
            },
            hyperParameters=hyper_parameters
        )

        job_arn = response["jobArn"]
        print(f"\n  âœ… Job submitted successfully!")
        print(f"  ğŸ“Œ Job ARN: {job_arn}")

        # Save job details for later use
        job_details = {
            "job_name": job_name,
            "job_arn": job_arn,
            "custom_model_name": custom_model_name,
            "base_model_id": BASE_MODEL_ID,
            "submitted_at": datetime.now().isoformat(),
            "bucket": BUCKET_NAME
        }

        with open("job_details.json", "w") as f:
            json.dump(job_details, f, indent=2)

        print(f"  ğŸ’¾ Job details saved to: job_details.json")

        # Monitor the job
        print(f"\n  â³ Monitoring job status (press Ctrl+C to stop monitoring)...")
        print(f"     Fine-tuning typically takes 30 minutes to 2+ hours.\n")

        while True:
            status_response = bedrock.get_model_customization_job(jobIdentifier=job_arn)
            status = status_response["status"]
            elapsed = datetime.now().strftime("%H:%M:%S")

            if status == "Completed":
                print(f"  [{elapsed}] âœ… COMPLETED â€” Your fine-tuned model is ready!")
                print(f"\n  ğŸ‰ Custom Model Name: {custom_model_name}")

                # Update job details
                job_details["status"] = "Completed"
                job_details["completed_at"] = datetime.now().isoformat()
                with open("job_details.json", "w") as f:
                    json.dump(job_details, f, indent=2)
                break

            elif status == "Failed":
                failure_message = status_response.get("failureMessage", "Unknown error")
                print(f"  [{elapsed}] âŒ FAILED â€” {failure_message}")
                break

            elif status == "Stopped":
                print(f"  [{elapsed}] â¹ï¸  STOPPED â€” Job was manually cancelled")
                break

            else:
                print(f"  [{elapsed}] â³ Status: {status}...")
                time.sleep(60)  # Check every 60 seconds

    except Exception as e:
        print(f"\n  âŒ Error: {e}")
        print(f"\n  ğŸ’¡ Troubleshooting tips:")
        print(f"     1. Verify your bucket name is correct")
        print(f"     2. Ensure the IAM role has the correct permissions")
        print(f"     3. Check that model access is enabled in Bedrock console")
        print(f"     4. Verify the training data file exists in S3")

    print("\n" + "=" * 55)

if __name__ == "__main__":
    start_fine_tuning_job()
```

Run it:

```bash
python start_finetuning.py
```

> **While you wait:** Fine-tuning takes time (30 minutes to 2+ hours). This is normal â€” the model is literally learning from your examples. Use this time to read ahead to Lab 3 and Lab 4, or take a well-deserved break.

#### What You Just Accomplished

You submitted a **custom model training job** to Amazon Bedrock. Behind the scenes, AWS is spinning up specialised hardware, loading the Titan foundation model, and adjusting its neural network weights based on your banking examples. When it finishes, you'll have a model that's better at banking-related conversations than the generic version.

---

### Lab 3 â€” Automate Retraining Using EventBridge + Step Functions

**Time:** ~2.5 hours  
**Goal:** Build an automated pipeline that retrains your model when new data arrives  
**Prerequisite:** Labs 1 and 2 completed

This is where it gets powerful. Instead of manually running scripts, we'll build a system that **automatically detects new training data and kicks off the entire fine-tuning pipeline**.

#### Architecture Overview

Here's what we're building:

```
New file uploaded      EventBridge          Step Functions         Bedrock
to S3 bucket      â†’   detects the      â†’   orchestrates the  â†’   fine-tunes
                       upload event         pipeline steps         the model
```

#### Step 1 â€” Create the Step Functions State Machine

The state machine defines our pipeline. Create `step_function_definition.json`:

```json
{
    "Comment": "Bedrock Fine-Tuning Automation Pipeline",
    "StartAt": "ValidateTrainingData",
    "States": {
        "ValidateTrainingData": {
            "Type": "Task",
            "Resource": "arn:aws:states:::lambda:invoke",
            "Parameters": {
                "FunctionName": "validate-training-data",
                "Payload": {
                    "bucket.$": "$.bucket",
                    "key.$": "$.key"
                }
            },
            "ResultPath": "$.validation",
            "Next": "CheckValidation",
            "Catch": [
                {
                    "ErrorEquals": ["States.ALL"],
                    "Next": "NotifyFailure",
                    "ResultPath": "$.error"
                }
            ]
        },
        "CheckValidation": {
            "Type": "Choice",
            "Choices": [
                {
                    "Variable": "$.validation.Payload.isValid",
                    "BooleanEquals": true,
                    "Next": "StartFineTuningJob"
                }
            ],
            "Default": "NotifyValidationFailed"
        },
        "StartFineTuningJob": {
            "Type": "Task",
            "Resource": "arn:aws:states:::lambda:invoke",
            "Parameters": {
                "FunctionName": "start-bedrock-finetuning",
                "Payload": {
                    "bucket.$": "$.bucket",
                    "key.$": "$.key",
                    "validation.$": "$.validation"
                }
            },
            "ResultPath": "$.finetuning",
            "Next": "WaitForTraining"
        },
        "WaitForTraining": {
            "Type": "Wait",
            "Seconds": 300,
            "Next": "CheckTrainingStatus"
        },
        "CheckTrainingStatus": {
            "Type": "Task",
            "Resource": "arn:aws:states:::lambda:invoke",
            "Parameters": {
                "FunctionName": "check-finetuning-status",
                "Payload": {
                    "jobArn.$": "$.finetuning.Payload.jobArn"
                }
            },
            "ResultPath": "$.status",
            "Next": "IsTrainingComplete"
        },
        "IsTrainingComplete": {
            "Type": "Choice",
            "Choices": [
                {
                    "Variable": "$.status.Payload.status",
                    "StringEquals": "Completed",
                    "Next": "RunEvaluation"
                },
                {
                    "Variable": "$.status.Payload.status",
                    "StringEquals": "Failed",
                    "Next": "NotifyFailure"
                }
            ],
            "Default": "WaitForTraining"
        },
        "RunEvaluation": {
            "Type": "Task",
            "Resource": "arn:aws:states:::lambda:invoke",
            "Parameters": {
                "FunctionName": "evaluate-finetuned-model",
                "Payload": {
                    "customModelName.$": "$.finetuning.Payload.customModelName",
                    "baseModelId": "amazon.titan-text-express-v1"
                }
            },
            "ResultPath": "$.evaluation",
            "Next": "RegisterModel"
        },
        "RegisterModel": {
            "Type": "Task",
            "Resource": "arn:aws:states:::lambda:invoke",
            "Parameters": {
                "FunctionName": "register-model-version",
                "Payload": {
                    "customModelName.$": "$.finetuning.Payload.customModelName",
                    "evaluation.$": "$.evaluation.Payload"
                }
            },
            "ResultPath": "$.registry",
            "Next": "NotifySuccess"
        },
        "NotifySuccess": {
            "Type": "Task",
            "Resource": "arn:aws:states:::sns:publish",
            "Parameters": {
                "TopicArn": "arn:aws:sns:ap-south-1:ACCOUNT_ID:finetuning-notifications",
                "Subject": "Fine-Tuning Complete",
                "Message.$": "States.Format('Model {} training complete. Evaluation score: {}', $.finetuning.Payload.customModelName, $.evaluation.Payload.overallScore)"
            },
            "End": true
        },
        "NotifyValidationFailed": {
            "Type": "Task",
            "Resource": "arn:aws:states:::sns:publish",
            "Parameters": {
                "TopicArn": "arn:aws:sns:ap-south-1:ACCOUNT_ID:finetuning-notifications",
                "Subject": "Training Data Validation Failed",
                "Message": "Uploaded training data failed validation checks. Please review the data format."
            },
            "End": true
        },
        "NotifyFailure": {
            "Type": "Task",
            "Resource": "arn:aws:states:::sns:publish",
            "Parameters": {
                "TopicArn": "arn:aws:sns:ap-south-1:ACCOUNT_ID:finetuning-notifications",
                "Subject": "Fine-Tuning Pipeline Failed",
                "Message.$": "States.Format('Pipeline failed with error: {}', $.error.Cause)"
            },
            "End": true
        }
    }
}
```

#### Step 2 â€” Deploy the Automation Setup Script

Create `deploy_automation.py`:

```python
import boto3
import json
import time

# â”€â”€â”€ CONFIGURATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
REGION = "ap-south-1"
BUCKET_NAME = "bedrock-finetuning-your-unique-name"  # â† Change this!
ACCOUNT_ID = None  # We'll detect this automatically
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_account_id():
    """Automatically detect the AWS account ID."""
    sts = boto3.client("sts")
    return sts.get_caller_identity()["Account"]

def create_eventbridge_rule():
    """Create an EventBridge rule that triggers when new training data is uploaded."""

    events = boto3.client("events", region_name=REGION)

    # This rule watches for new .jsonl files in our training-data folder
    event_pattern = {
        "source": ["aws.s3"],
        "detail-type": ["Object Created"],
        "detail": {
            "bucket": {"name": [BUCKET_NAME]},
            "object": {
                "key": [{
                    "prefix": "training-data/",
                    "suffix": ".jsonl"
                }]
            }
        }
    }

    print("  ğŸ“¡ Creating EventBridge rule...")

    response = events.put_rule(
        Name="bedrock-finetuning-trigger",
        Description="Triggers fine-tuning pipeline when new training data is uploaded to S3",
        EventPattern=json.dumps(event_pattern),
        State="ENABLED"
    )

    print(f"  âœ… EventBridge rule created: {response['RuleArn']}")
    return response["RuleArn"]

def enable_s3_event_notifications():
    """Enable S3 to send events to EventBridge."""

    s3 = boto3.client("s3", region_name=REGION)

    print("  ğŸª£ Enabling S3 EventBridge notifications...")

    s3.put_bucket_notification_configuration(
        Bucket=BUCKET_NAME,
        NotificationConfiguration={
            "EventBridgeConfiguration": {}
        }
    )

    print("  âœ… S3 EventBridge notifications enabled")

def create_sns_topic():
    """Create an SNS topic for pipeline notifications."""

    sns = boto3.client("sns", region_name=REGION)

    print("  ğŸ“§ Creating SNS notification topic...")

    response = sns.create_topic(Name="finetuning-notifications")
    topic_arn = response["TopicArn"]

    print(f"  âœ… SNS topic created: {topic_arn}")
    print(f"\n  ğŸ“¬ To receive notifications, subscribe your email:")
    print(f"     aws sns subscribe --topic-arn {topic_arn} \\")
    print(f"       --protocol email --notification-endpoint YOUR_EMAIL@example.com")

    return topic_arn

def deploy():
    """Deploy the complete automation infrastructure."""

    global ACCOUNT_ID

    print("=" * 60)
    print("  Bedrock Fine-Tuning â€” Automation Deployment")
    print("=" * 60)

    try:
        ACCOUNT_ID = get_account_id()
    except Exception as e:
        print(f"\n  âŒ AWS Connection Failed: {e}")
        print(f"\n  ğŸ’¡ Make sure you have configured AWS credentials:")
        print(f"     Run: aws configure")
        print("\n" + "=" * 60)
        return

    print(f"\n  ğŸŒ Region      : {REGION}")
    print(f"  ğŸª£ Bucket      : {BUCKET_NAME}")
    print(f"  ğŸ”¢ Account ID  : {ACCOUNT_ID}")
    print()

    try:
        # Step 1: Enable S3 event notifications
        enable_s3_event_notifications()
        print()

        # Step 2: Create EventBridge rule
        rule_arn = create_eventbridge_rule()
        print()

        # Step 3: Create SNS notification topic
        topic_arn = create_sns_topic()
        print()
    except Exception as e:
        print(f"\n  âŒ Deployment Failed: {e}")
        print(f"\n  ğŸ’¡ Troubleshooting tips:")
        print(f"     1. Verify your S3 bucket exists: aws s3 ls s3://{BUCKET_NAME}")
        print(f"     2. Check your IAM permissions include EventBridge and SNS access")
        print(f"     3. Verify the region is correct: {REGION}")
        print("\n" + "=" * 60)
        return

    print("=" * 60)
    print("  âœ… Automation infrastructure deployed!")
    print("=" * 60)
    print(f"""
  What happens now:
  
  1. When you upload a new .jsonl file to:
     s3://{BUCKET_NAME}/training-data/
  
  2. EventBridge detects the upload and triggers
     the Step Functions pipeline
  
  3. The pipeline validates data â†’ fine-tunes model â†’
     evaluates results â†’ registers the model version
  
  4. You receive an email notification with the results

  To test it, upload a new training data file:
    aws s3 cp new_training_data.jsonl \\
      s3://{BUCKET_NAME}/training-data/new_training_data.jsonl
""")

    # Save deployment details
    deployment_info = {
        "account_id": ACCOUNT_ID,
        "region": REGION,
        "bucket": BUCKET_NAME,
        "eventbridge_rule_arn": rule_arn,
        "sns_topic_arn": topic_arn,
        "deployed_at": time.strftime("%Y-%m-%dT%H:%M:%SZ")
    }

    with open("deployment_info.json", "w") as f:
        json.dump(deployment_info, f, indent=2)

    print("  ğŸ’¾ Deployment details saved to: deployment_info.json")

if __name__ == "__main__":
    deploy()
```

Run it:

```bash
python deploy_automation.py
```

#### What You Just Accomplished

You built the foundation of an **event-driven automation pipeline**. EventBridge is now watching your S3 bucket, and when new training data arrives, it can trigger the Step Functions state machine that orchestrates the entire fine-tuning process. This is the same architectural pattern used by production ML teams at major enterprises.

> **Note:** The Step Functions state machine references Lambda functions that we haven't created (this is intentional â€” they're beyond the scope of this beginner tutorial). In production, you'd implement these Lambda functions to handle validation, status checking, evaluation, and model registration. The state machine shown here is a teaching tool to demonstrate the automation pattern.

---

### Lab 4 â€” Compare Baseline vs Fine-Tuned Outputs

**Time:** ~2 hours (including PT setup)  
**Goal:** Run the same prompts through both the original and fine-tuned models, then objectively compare  
**Prerequisite:** Lab 2 completed (fine-tuning job finished successfully)

This is the "moment of truth" lab â€” where you measure whether all that training actually made a difference.

#### IMPORTANT: Set Up Model Inference First

Custom models cannot be invoked directly by name. You must set up inference capability before testing. Choose one option:

**Option A: Provisioned Throughput (Recommended for Testing)**

Provisioned Throughput gives you dedicated compute with guaranteed throughput. For learning/testing, we'll use the "no commitment" option (hourly billing).

**Console Method**:

1. Open the [Amazon Bedrock Console](https://console.aws.amazon.com/bedrock/)
2. Navigate to **Custom models** â†’ **Provisioned Throughput**
3. Click **Purchase Provisioned Throughput**
4. Configure:
   - **Provisioned Throughput name**: `my-finetuned-model-pt`
   - **Select model**: Choose your custom model from the dropdown
   - **Commitment term**: Select **No commitment** (hourly billing)
   - **Model Units**: Set to **1** (minimum for testing)
5. Click **Purchase Provisioned Throughput**
6. **Acknowledge** the pricing and commitment notice
7. Wait for **Status** to show **InService** (~5-10 minutes)
8. Copy the **Provisioned Model ARN** (it will look like: `arn:aws:bedrock:ap-south-1:123456789012:provisioned-model/abc123xyz456`)

**CLI Method**:

```bash
# Load your custom model name from the saved job details
CUSTOM_MODEL_NAME=$(python -c "import json; print(json.load(open('job_details.json'))['custom_model_name'])")

# Create Provisioned Throughput (no commitment)
aws bedrock create-provisioned-model-throughput \
    --provisioned-model-name my-finetuned-model-pt \
    --model-id $CUSTOM_MODEL_NAME \
    --model-units 1 \
    --region ap-south-1

# Wait for it to be ready (check status)
aws bedrock get-provisioned-model-throughput \
    --provisioned-model-id my-finetuned-model-pt \
    --region ap-south-1
```

> **ğŸ’° Cost Warning**: Provisioned Throughput costs approximately **$8-15 per hour** depending on the base model and region. For this learning lab:
> - Purchase PT
> - Run evaluation (takes ~5 minutes)
> - **Immediately delete PT** after completing the lab
> - Total cost: ~$1-2 if you're quick!

**Option B: On-Demand Deployment (If Available)**

Some accounts may have access to on-demand custom model deployment (newer feature):

```bash
# Check if on-demand deployment is available
aws bedrock create-model-customization-deployment \
    --custom-model-name $CUSTOM_MODEL_NAME \
    --deployment-name my-model-deployment \
    --region ap-south-1
```

If successful, use the returned deployment ARN instead of the provisioned model ARN.

#### The Evaluation Script

Create `evaluate_models.py`:

```python
import boto3
import json
from datetime import datetime

# â”€â”€â”€ CONFIGURATION â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
REGION = "ap-south-1"
BASE_MODEL_ID = "amazon.titan-text-express-v1"

# CRITICAL: Replace with YOUR Provisioned Model ARN from the setup step above
# Find this in: Bedrock Console â†’ Custom models â†’ Provisioned Throughput â†’ Copy ARN
PROVISIONED_MODEL_ARN = "arn:aws:bedrock:ap-south-1:ACCOUNT_ID:provisioned-model/XXXXX"
# Example: "arn:aws:bedrock:ap-south-1:123456789012:provisioned-model/abc123def456"
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def validate_configuration():
    """Ensure the user has updated the Provisioned Model ARN."""
    if "XXXXX" in PROVISIONED_MODEL_ARN or "ACCOUNT_ID" in PROVISIONED_MODEL_ARN:
        print("=" * 65)
        print("  âš ï¸  CONFIGURATION ERROR")
        print("=" * 65)
        print()
        print("  You must update PROVISIONED_MODEL_ARN in this script!")
        print()
        print("  Steps:")
        print("  1. Open Bedrock Console â†’ Custom models â†’ Provisioned Throughput")
        print("  2. Find your provisioned throughput (my-finetuned-model-pt)")
        print("  3. Copy the 'Provisioned model ARN'")
        print("  4. Paste it into the PROVISIONED_MODEL_ARN variable at the top")
        print("     of this script")
        print()
        print("=" * 65)
        exit(1)

def invoke_base_model(bedrock_runtime, prompt):
    """Send a prompt to the base (original) model."""
    body = json.dumps({
        "inputText": prompt,
        "textGenerationConfig": {
            "maxTokenCount": 300,
            "temperature": 0.1,
            "topP": 0.9
        }
    })

    try:
        response = bedrock_runtime.invoke_model(
            modelId=BASE_MODEL_ID,
            contentType="application/json",
            accept="application/json",
            body=body
        )

        result = json.loads(response["body"].read())
        return result["results"][0]["outputText"].strip()
    except Exception as e:
        return f"[Error invoking base model: {e}]"

def invoke_finetuned_model(bedrock_runtime, provisioned_model_arn, prompt):
    """Send a prompt to the fine-tuned model via Provisioned Throughput."""
    body = json.dumps({
        "inputText": prompt,
        "textGenerationConfig": {
            "maxTokenCount": 300,
            "temperature": 0.1,
            "topP": 0.9
        }
    })

    try:
        response = bedrock_runtime.invoke_model(
            modelId=provisioned_model_arn,  # Use the PT ARN
            contentType="application/json",
            accept="application/json",
            body=body
        )
        result = json.loads(response["body"].read())
        return result["results"][0]["outputText"].strip()
    except Exception as e:
        return f"[Error invoking fine-tuned model: {e}]"

def evaluate():
    """Compare base model vs fine-tuned model responses."""

    # Validate configuration first
    validate_configuration()

    bedrock_runtime = boto3.client("bedrock-runtime", region_name=REGION)

    print("=" * 65)
    print("  Model Evaluation â€” Baseline vs Fine-Tuned Comparison")
    print("=" * 65)
    print(f"\n  ğŸ§  Base Model      : {BASE_MODEL_ID}")
    print(f"  ğŸ¯ Fine-Tuned Model: {PROVISIONED_MODEL_ARN}")
    print()
    print("  ğŸ’¡ Tip: Look for differences in:")
    print("     â€¢ Specificity and detail")
    print("     â€¢ Structured response format")
    print("     â€¢ Domain-appropriate language")
    print("     â€¢ Consistency with training examples")

    # Test prompts â€” a mix of in-domain (banking) and out-of-domain queries
    test_prompts = [
        {
            "prompt": "What is the process to dispute a credit card transaction?",
            "category": "In-Domain (trained on similar example)",
            "expected_theme": "Steps for disputing transactions"
        },
        {
            "prompt": "How do I reset my online banking password?",
            "category": "In-Domain (related topic, not in training data)",
            "expected_theme": "Password reset process"
        },
        {
            "prompt": "What are the current mortgage rates?",
            "category": "In-Domain (banking, not in training data)",
            "expected_theme": "Current rate information"
        },
        {
            "prompt": "Can you explain how blockchain technology works?",
            "category": "Out-of-Domain (general tech question)",
            "expected_theme": "Technical explanation"
        }
    ]

    results = []

    for i, test in enumerate(test_prompts, 1):
        print(f"\n{'â”€' * 65}")
        print(f"  Test {i}/{len(test_prompts)}: {test['category']}")
        print(f"  Prompt: \"{test['prompt']}\"")
        print(f"{'â”€' * 65}")

        # Get base model response
        print(f"\n  ğŸŸ¦ BASE MODEL response:")
        base_response = invoke_base_model(bedrock_runtime, test["prompt"])
        print(f"  {base_response[:200]}{'...' if len(base_response) > 200 else ''}")

        # Get fine-tuned model response
        print(f"\n  ğŸŸ© FINE-TUNED MODEL response:")
        ft_response = invoke_finetuned_model(
            bedrock_runtime, PROVISIONED_MODEL_ARN, test["prompt"]
        )
        print(f"  {ft_response[:200]}{'...' if len(ft_response) > 200 else ''}")

        results.append({
            "prompt": test["prompt"],
            "category": test["category"],
            "base_response": base_response,
            "finetuned_response": ft_response,
            "expected_theme": test["expected_theme"]
        })

    # Save full results
    evaluation_report = {
        "evaluation_date": datetime.now().isoformat(),
        "base_model": BASE_MODEL_ID,
        "finetuned_model_arn": PROVISIONED_MODEL_ARN,
        "results": results,
        "summary": {
            "total_tests": len(test_prompts),
            "in_domain_tests": sum(
                1 for t in test_prompts if "In-Domain" in t["category"]
            ),
            "out_of_domain_tests": sum(
                1 for t in test_prompts if "Out-of-Domain" in t["category"]
            )
        }
    }

    with open("evaluation_report.json", "w") as f:
        json.dump(evaluation_report, f, indent=2)

    print(f"\n{'=' * 65}")
    print(f"  ğŸ“Š Evaluation Complete!")
    print(f"{'=' * 65}")
    print(f"\n  ğŸ“ Full report saved to: evaluation_report.json")
    print(f"\n  ğŸ“ˆ What to look for:")
    print(f"     â€¢ In-Domain prompts: Fine-tuned model should give more")
    print(f"       specific, banking-appropriate responses")
    print(f"     â€¢ Out-of-Domain prompts: Both models should perform")
    print(f"       similarly (fine-tuning shouldn't hurt general ability)")
    print(f"     â€¢ Response format: Fine-tuned model may follow the")
    print(f"       structured format from our training examples")
    print(f"\n  ğŸ’¡ Tip: Open evaluation_report.json to read full responses")
    print(f"     side-by-side for detailed comparison.")
    print()
    print(f"  ğŸ§¹ IMPORTANT: After reviewing results, DELETE your")
    print(f"     Provisioned Throughput to stop billing!")
    print(f"     See cleanup instructions below.")

if __name__ == "__main__":
    evaluate()
```

Run it:

```bash
# BEFORE running, edit the script and update PROVISIONED_MODEL_ARN
python evaluate_models.py
```

#### What You Just Accomplished

You built an **automated evaluation pipeline** that objectively compares your fine-tuned model against the baseline. This is a fundamental practice in ML engineering â€” you should never deploy a model without evidence that it's better than what you had before.

---

## Cleanup â€” Avoid Unexpected Charges

When you're done learning, clean up your resources to stop incurring AWS charges. **Complete these steps in order**:

### Step 1: Delete Provisioned Throughput (CRITICAL - DO FIRST)

**This is the most expensive resource and bills hourly!**

**Console Method**:
1. Open Bedrock Console â†’ **Custom models** â†’ **Provisioned Throughput**
2. Select your provisioned throughput (`my-finetuned-model-pt`)
3. Click **Delete**
4. Confirm deletion

**CLI Method**:
```bash
# List all provisioned throughputs to confirm name
aws bedrock list-provisioned-model-throughputs --region ap-south-1

# Delete by name
aws bedrock delete-provisioned-model-throughput \
    --provisioned-model-id my-finetuned-model-pt \
    --region ap-south-1
```

> **Verification**: Wait 2-3 minutes, then verify deletion:
> ```bash
> aws bedrock list-provisioned-model-throughputs --region ap-south-1
> ```
> Should return empty or not include your PT.

### Step 2: Delete S3 Resources

```bash
# Delete all objects in the bucket first
aws s3 rm s3://bedrock-finetuning-your-unique-name --recursive

# Then delete the bucket
aws s3 rb s3://bedrock-finetuning-your-unique-name
```

### Step 3: Delete IAM Role and Policies

```bash
# Delete the inline policy first
aws iam delete-role-policy \
    --role-name BedrockFineTuningRole \
    --policy-name BedrockS3Access

# Then delete the role
aws iam delete-role --role-name BedrockFineTuningRole
```

### Step 4: Delete EventBridge Rule (if created in Lab 3)

```bash
# Remove targets first
aws events remove-targets \
    --rule bedrock-finetuning-trigger \
    --ids "1" \
    --region ap-south-1 2>/dev/null

# Then delete the rule
aws events delete-rule \
    --name bedrock-finetuning-trigger \
    --region ap-south-1 2>/dev/null
```

### Step 5: Delete SNS Topic (if created in Lab 3)

```bash
# List topics to find the ARN
aws sns list-topics \
    --query "Topics[?contains(TopicArn, 'finetuning-notifications')]" \
    --output text

# Delete using the ARN (replace with actual ARN from above)
aws sns delete-topic \
    --topic-arn arn:aws:sns:ap-south-1:ACCOUNT_ID:finetuning-notifications
```

### Step 6: (Optional) Delete Custom Model

**Note**: Custom models themselves don't incur charges (only Provisioned Throughput does), but you can delete them to keep your account clean.

**Console Method**:
1. Bedrock Console â†’ **Custom models** â†’ **Models**
2. Select your custom model
3. Click **Delete**

**CLI Method**:
```bash
# Load custom model name from saved job details
CUSTOM_MODEL_NAME=$(python -c "import json; print(json.load(open('job_details.json'))['custom_model_name'])")

# Delete the model
aws bedrock delete-custom-model \
    --model-identifier $CUSTOM_MODEL_NAME \
    --region ap-south-1
```

### Cleanup Verification

Run this script to verify all resources are deleted:

```bash
#!/bin/bash
echo "=== Cleanup Verification ==="
echo ""
echo "Checking Provisioned Throughput..."
aws bedrock list-provisioned-model-throughputs --region ap-south-1 --query "provisionedModelSummaries[?contains(provisionedModelName, 'finetuned')]"
echo ""
echo "Checking S3 Buckets..."
aws s3 ls | grep finetuning
echo ""
echo "Checking IAM Roles..."
aws iam get-role --role-name BedrockFineTuningRole 2>&1 | grep -q "NoSuchEntity" && echo "âœ… Role deleted" || echo "âš ï¸ Role still exists"
echo ""
echo "=== Verification Complete ==="
```

> **Final Cost Check**: After cleanup, monitor your AWS Billing Console for 24-48 hours to ensure no unexpected charges appear.

---

## Quick Reference â€” Key AWS CLI Commands

| Action | Command |
|---|---|
| List Bedrock models | `aws bedrock list-foundation-models --region ap-south-1` |
| Check fine-tuning jobs | `aws bedrock list-model-customization-jobs --region ap-south-1` |
| Get job status | `aws bedrock get-model-customization-job --job-identifier JOB_ARN` |
| List custom models | `aws bedrock list-custom-models --region ap-south-1` |
| **List Provisioned Throughput** | `aws bedrock list-provisioned-model-throughputs --region ap-south-1` |
| **Delete Provisioned Throughput** | `aws bedrock delete-provisioned-model-throughput --provisioned-model-id NAME` |
| List S3 buckets | `aws s3 ls` |
| List files in bucket | `aws s3 ls s3://your-bucket-name/` |
| Check IAM role | `aws iam get-role --role-name BedrockFineTuningRole` |

---

## Troubleshooting

### "AccessDeniedException" when starting fine-tuning
Your IAM role doesn't have the right permissions. Re-run the IAM setup commands in Lab 2, making sure the S3 bucket name in the policy matches your actual bucket.

### "ResourceNotFoundException" when invoking custom model
You haven't set up inference yet. Custom models require either Provisioned Throughput or on-demand deployment before they can be invoked. See Lab 4 prerequisites.

### "ThrottlingException" or rate limits
Provisioned Throughput is still being created. Wait for status to show "InService" in the console (5-10 minutes).

### "ValidationException: Use case not submitted" (Anthropic models)
First-time Anthropic users must submit use case details. Go to Model Catalog â†’ Select Claude model â†’ Submit the one-time form. Access is granted immediately.

### Fine-tuning job stuck in "InProgress" for many hours
Jobs can take 1â€“4+ hours depending on dataset size. If it's been over 6 hours with no progress, check the job details for error messages.

### "ValidationException" on training data
Your JSONL file has formatting issues. Common fixes: ensure every line is valid JSON with exactly `prompt` and `completion` fields, no trailing commas, and no empty lines.

### EventBridge rule not triggering
Verify S3 EventBridge notifications are enabled on your bucket. Run the `enable_s3_event_notifications()` function from Lab 3 again.

### Provisioned Throughput shows "Failed" status
Check the failure message in the console. Common causes: insufficient service quota (Model Units), invalid model ID, or account limitations. Contact AWS Support to request Model Unit quota increase.

### High AWS charges after completing labs
**Most likely cause**: Provisioned Throughput not deleted. Even when not in use, PT bills hourly. Delete immediately after testing. Check AWS Cost Explorer â†’ Filter by "Bedrock" service.

---

## What's Next?

Once you're comfortable with these labs, explore:

- **Advanced Evaluation** â€” Use Claude to auto-score fine-tuned responses on relevance, accuracy, and safety
- **A/B Testing** â€” Route production traffic between baseline and fine-tuned models to measure real-world impact
- **Continuous Training** â€” Set up weekly automated retraining with fresh customer interaction data
- **Multi-Model Strategy** â€” Fine-tune different models for different use cases (e.g., one for customer support, one for document summarisation)
- **Guardrails Integration** â€” Add Bedrock Guardrails to ensure fine-tuned models remain safe and compliant

---

## Resources

- [Amazon Bedrock Documentation](https://docs.aws.amazon.com/bedrock/)
- [Bedrock Custom Models Guide](https://docs.aws.amazon.com/bedrock/latest/userguide/custom-models.html)
- [AWS Step Functions Developer Guide](https://docs.aws.amazon.com/step-functions/latest/dg/)
- [Amazon EventBridge User Guide](https://docs.aws.amazon.com/eventbridge/latest/userguide/)
- [Bedrock Pricing](https://aws.amazon.com/bedrock/pricing/)

---

## Version History

- **v2.0 (February 2026)** â€” Updated for simplified model access, Provisioned Throughput requirements, and current Bedrock APIs
- **v1.0 (Original)** â€” Initial release

---

> **Built for learners.** If something in this guide was confusing or didn't work, that's our problem to fix â€” not yours. Open an issue and let us know.
